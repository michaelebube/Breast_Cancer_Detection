{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b1e630",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn modules\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05539bc",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wisconsin Breast Cancer Dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"üìä Dataset Shape:\", df.shape)\n",
    "print(\"\\nüìã Feature Names:\")\n",
    "for i, name in enumerate(data.feature_names):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "\n",
    "print(\"\\nüéØ Target Classes:\")\n",
    "print(f\"  0 = {data.target_names[0]} (Malignant)\")\n",
    "print(f\"  1 = {data.target_names[1]} (Benign)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nüìù First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"üìà Dataset Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nüîç Missing Values:\")\n",
    "print(df.isnull().sum().sum(), \"missing values found\")\n",
    "\n",
    "# Target distribution\n",
    "print(\"\\nüìä Target Distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nBenign: {(df['target'] == 1).sum()} ({(df['target'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"Malignant: {(df['target'] == 0).sum()} ({(df['target'] == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167cba4",
   "metadata": {},
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdec4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "axes[0].pie(df['target'].value_counts(), labels=['Benign', 'Malignant'], \n",
    "            autopct='%1.1f%%', colors=colors, explode=(0.05, 0))\n",
    "axes[0].set_title('Target Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "df['target'].value_counts().plot(kind='bar', ax=axes[1], color=colors)\n",
    "axes[1].set_title('Target Class Count', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(['Benign (1)', 'Malignant (0)'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f297201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for mean features\n",
    "mean_features = [col for col in df.columns if 'mean' in col]\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[mean_features + ['target']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Correlation Heatmap (Mean Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by target class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(mean_features):\n",
    "    for target_val, color, label in [(0, '#ff6b6b', 'Malignant'), (1, '#4ecdc4', 'Benign')]:\n",
    "        axes[idx].hist(df[df['target'] == target_val][feature], \n",
    "                      alpha=0.6, bins=20, color=color, label=label)\n",
    "    axes[idx].set_title(feature.replace('_', ' ').title(), fontsize=10)\n",
    "    axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Target Class', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461459d1",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"üìä Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b222b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Feature scaling completed!\")\n",
    "print(f\"\\nüìà Scaled feature statistics:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a7c86",
   "metadata": {},
   "source": [
    "## 5. Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab55000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "print(\"üîÑ Training and evaluating models...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {auc:.4f}\")\n",
    "    print(f\"  CV Score:  {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models visually\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1 Score': [r['f1'] for r in results.values()],\n",
    "    'AUC-ROC': [r['auc'] for r in results.values()]\n",
    "})\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metrics_df['Model']))\n",
    "width = 0.15\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax.bar(x + i * width, metrics_df[metric], width, label=metric, color=color)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(metrics_df['Model'], rotation=15)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0.8, 1.02)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd901b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "for (name, result), color in zip(results.items(), colors):\n",
    "    model = result['model']\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, color=color, lw=2, \n",
    "             label=f'{name} (AUC = {result[\"auc\"]:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1a0f5",
   "metadata": {},
   "source": [
    "## 6. Select Best Model & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16becea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1 score\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1'])\n",
    "print(f\"üèÜ Best performing model: {best_model_name}\")\n",
    "print(f\"   F1 Score: {results[best_model_name]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best performing model (Logistic Regression)\n",
    "# No hyperparameter tuning needed - LR works great with default params on this dataset\n",
    "print(\"üèÜ Using Logistic Regression as the final model (best performer)\")\n",
    "\n",
    "final_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Final evaluation\n",
    "y_pred_final = final_model.predict(X_test_scaled)\n",
    "y_pred_proba_final = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\nüìä Final Model Performance on Test Set:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"  AUC-ROC:   {roc_auc_score(y_test, y_pred_proba_final):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix - Final Model', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014713a",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances (using model coefficients for Logistic Regression)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data.feature_names,\n",
    "    'importance': np.abs(final_model.coef_[0])  # Use absolute coefficients\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(top_features)))\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient Magnitude', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 15 Feature Importances (Logistic Regression)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43694ca4",
   "metadata": {},
   "source": [
    "## 8. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dataset statistics for feature mapping\n",
    "dataset_stats = {\n",
    "    'feature_names': list(data.feature_names),\n",
    "    'feature_means': X.mean().to_dict(),\n",
    "    'feature_stds': X.std().to_dict(),\n",
    "    'feature_mins': X.min().to_dict(),\n",
    "    'feature_maxs': X.max().to_dict(),\n",
    "    'scaler_mean': scaler.mean_.tolist(),\n",
    "    'scaler_scale': scaler.scale_.tolist()\n",
    "}\n",
    "\n",
    "print(\"üìä Dataset Statistics Calculated\")\n",
    "print(f\"   Number of features: {len(dataset_stats['feature_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using joblib\n",
    "joblib.dump(final_model, 'breast_cancer_model.joblib')\n",
    "print(\"‚úÖ Model saved: breast_cancer_model.joblib\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"‚úÖ Scaler saved: scaler.joblib\")\n",
    "\n",
    "# Save dataset statistics as JSON\n",
    "with open('dataset_stats.json', 'w') as f:\n",
    "    json.dump(dataset_stats, f, indent=2)\n",
    "print(\"‚úÖ Dataset statistics saved: dataset_stats.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved model\n",
    "print(\"\\nüîç Verifying saved model...\")\n",
    "\n",
    "# Load and test\n",
    "loaded_model = joblib.load('breast_cancer_model.joblib')\n",
    "loaded_scaler = joblib.load('scaler.joblib')\n",
    "\n",
    "# Test prediction\n",
    "test_sample = X_test.iloc[0:1]\n",
    "test_sample_scaled = loaded_scaler.transform(test_sample)\n",
    "prediction = loaded_model.predict(test_sample_scaled)\n",
    "probability = loaded_model.predict_proba(test_sample_scaled)\n",
    "\n",
    "print(f\"\\nüìù Test Prediction:\")\n",
    "print(f\"   Prediction: {'Benign' if prediction[0] == 1 else 'Malignant'}\")\n",
    "print(f\"   Confidence: {max(probability[0]) * 100:.2f}%\")\n",
    "print(f\"   Actual: {'Benign' if y_test.iloc[0] == 1 else 'Malignant'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e4fb2",
   "metadata": {},
   "source": [
    "## 9. Download Model Files\n",
    "\n",
    "Run the cell below to download the model files to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (for Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"üì• Downloading model files...\")\n",
    "    files.download('breast_cancer_model.joblib')\n",
    "    files.download('scaler.joblib')\n",
    "    files.download('dataset_stats.json')\n",
    "    print(\"\\n‚úÖ All files downloaded successfully!\")\n",
    "    print(\"\\nüìÅ Place these files in your backend/model/ directory\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Not running in Google Colab.\")\n",
    "    print(\"üìÅ Files are saved in the current directory:\")\n",
    "    print(\"   - breast_cancer_model.joblib\")\n",
    "    print(\"   - scaler.joblib\")\n",
    "    print(\"   - dataset_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b430a73",
   "metadata": {},
   "source": [
    "## 10. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b22875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"           ü©∫ BREAST CANCER DETECTION MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Dataset: Wisconsin Breast Cancer Dataset\")\n",
    "print(f\"   - Total samples: {len(df)}\")\n",
    "print(f\"   - Features: {len(data.feature_names)}\")\n",
    "print(f\"   - Classes: Benign (1), Malignant (0)\")\n",
    "print(f\"\\nü§ñ Model: Logistic Regression\")\n",
    "print(f\"\\nüìà Performance Metrics:\")\n",
    "print(f\"   - Accuracy:  {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"   - Precision: {precision_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"   - Recall:    {recall_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"   - F1 Score:  {f1_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"   - AUC-ROC:   {roc_auc_score(y_test, y_pred_proba_final):.4f}\")\n",
    "print(f\"\\nüíæ Saved Files:\")\n",
    "print(f\"   - breast_cancer_model.joblib (trained model)\")\n",
    "print(f\"   - scaler.joblib (feature scaler)\")\n",
    "print(f\"   - dataset_stats.json (feature statistics)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training complete! Model ready for deployment.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
